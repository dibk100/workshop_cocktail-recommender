from neo4j import GraphDatabase
from config import NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD

driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

class GraphRAG:
    """Graph RAG ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, neo4j_uri: str = None, neo4j_user: str = None, neo4j_password: str = None, model_id: str = None):
        """GraphRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        import os
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ì½ê¸° (íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´)
        neo4j_uri = neo4j_uri or os.getenv('NEO4J_URI')
        neo4j_user = neo4j_user or os.getenv('NEO4J_USER')
        neo4j_password = neo4j_password or os.getenv('NEO4J_PASSWORD')
        model_id = model_id or os.getenv('MODEL_ID')
        hf_token = os.getenv('HF_TOKEN')
        self.retrieval = Neo4jRetrieval(neo4j_uri, neo4j_user, neo4j_password)
        self.model_id = model_id
        
        # ë””ë²„ê·¸ ì •ë³´ ì €ì¥ìš©
        self.last_debug_info = {}
        
        base_model_path = model_id  # .envì—ì„œ ì½ì€ base_model ê²½ë¡œ
        adapter_path = "/home/shcho95/yjllm/llama3_8b/weight/perfume_llama3_8B_v0"  # ì–´ëŒ‘í„° ê²½ë¡œ

        model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            token=hf_token
        )
        model = PeftModel.from_pretrained(model, adapter_path)
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, token=hf_token)

        self.pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device_map="auto",
            torch_dtype=torch.float16
        )
        print("âœ… GraphRAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _extract_generated_response(self, full_output: str, prompt: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë‹µë³€ë§Œ ì¶”ì¶œ"""
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œê±°
        if prompt in full_output:
            response = full_output.replace(prompt, "").strip()
        else:
            response = full_output.strip()
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ë‚˜ ë°˜ë³µëœ ë‚´ìš© ì œê±°
        response = re.sub(r'<[^>]+>', '', response)  # HTML íƒœê·¸ ì œê±°
        response = re.sub(r'\n+', '\n', response)  # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        
        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
        if not response or len(response.strip()) < 10:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¶”ì²œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        return response.strip()
    
    def ask(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ í–¥ìˆ˜ ì¶”ì²œ ì‘ë‹µ ìƒì„±"""
        try:
            print("\n" + "="*80)
            print(f"ğŸ” ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: '{user_query}'")
            print("="*80)
            
            # Graph RAG ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
            print("ğŸ“Š 1ë‹¨ê³„: Graph RAG ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            graph_rag_response = self.retrieval.search_graph_rag(user_query)
            
            # ë””ë²„ê·¸ ì •ë³´ ì €ì¥
            self.last_debug_info = {
                'user_query': user_query,
                'graph_rag_response': graph_rag_response,
                'timestamp': str(torch.cuda.current_stream()) if torch.cuda.is_available() else 'CPU'
            }
            
            print("âœ… Graph RAG ê²€ìƒ‰ ì™„ë£Œ!")
            print("\n" + "-"*60)
            print("ğŸ“‹ LLMì—ê²Œ ì „ë‹¬ë  ìµœì¢… ì»¨í…ìŠ¤íŠ¸:")
            print("-"*60)
            print(graph_rag_response)
            print("-"*60)
            
            # 3. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‘ì„±
            print("\nğŸ¤– 2ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
            prompt = self._create_prompt(user_query, graph_rag_response)
            
            print("âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ!")
            print("\n" + "-"*60)
            print("ğŸ“ LLMì—ê²Œ ì „ë‹¬ë  ì „ì²´ í”„ë¡¬í”„íŠ¸:")
            print("-"*60)
            print(prompt)
            print("-"*60)
            
            # 4. LLM ì‘ë‹µ ìƒì„±
            print("\nğŸ§  3ë‹¨ê³„: LLM ì‘ë‹µ ìƒì„± ì¤‘...")
            outputs = self.pipeline(
                prompt,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.pipeline.tokenizer.eos_token_id
            )
            
            # 5. ì‘ë‹µ í›„ì²˜ë¦¬
            full_response = outputs[0]["generated_text"]
            clean_response = self._extract_generated_response(full_response, prompt)
            
            # ì¶”ê°€ ë””ë²„ê·¸ ì •ë³´ ì €ì¥
            self.last_debug_info.update({
                'final_prompt': prompt,
                'full_llm_response': full_response,
                'clean_response': clean_response
            })
            
            print("âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ!")
            print("\n" + "-"*60)
            print("ğŸ¤– LLM ì›ë³¸ ì‘ë‹µ (í›„ì²˜ë¦¬ ì „):")
            print("-"*60)
            print(full_response)
            print("-"*60)
            
            print("\n" + "-"*60)
            print("âœ¨ ìµœì¢… ì •ì œëœ ì‘ë‹µ:")
            print("-"*60)
            print(clean_response)
            print("-"*60)
            
            print("\n" + "="*80)
            print("ğŸ‰ ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ!")
            print("="*80 + "\n")
            
            return clean_response
            
        except Exception as e:
            error_msg = f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
            print(f"\n{error_msg}")
            print("="*80 + "\n")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í–¥ìˆ˜ ì¶”ì²œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    def ask_with_debug(self, user_query: str) -> Dict:
        """ë””ë²„ê·¸ ì •ë³´ì™€ í•¨ê»˜ í–¥ìˆ˜ ì¶”ì²œ ì‘ë‹µ ìƒì„± (Streamlitìš©)"""
        try:
            # Graph RAG ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
            graph_rag_response = self.retrieval.search_graph_rag(user_query)
            
            # 3. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‘ì„±
            prompt = self._create_prompt(user_query, graph_rag_response)
            
            # 4. LLM ì‘ë‹µ ìƒì„±
            outputs = self.pipeline(
                prompt,
                max_new_tokens=100,
                do_sample=False,
                temperature=0.0,
                # top_p=0.9,
                pad_token_id=self.pipeline.tokenizer.eos_token_id
            )
            
            # 5. ì‘ë‹µ í›„ì²˜ë¦¬
            full_response = outputs[0]["generated_text"]
            clean_response = self._extract_generated_response(full_response, prompt)
            
            # ë””ë²„ê·¸ ì •ë³´ì™€ í•¨ê»˜ ë°˜í™˜
            return {
                'response': clean_response,
                'debug_info': {
                    'user_query': user_query,
                    'graph_rag_context': graph_rag_response,
                    'final_prompt': prompt,
                    'full_llm_response': full_response
                }
            }
            
        except Exception as e:
            error_msg = f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
            print(error_msg)
            return {
                'response': "ì£„ì†¡í•©ë‹ˆë‹¤. í–¥ìˆ˜ ì¶”ì²œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                'debug_info': {
                    'user_query': user_query,
                    'error': str(e)
                }
            }
    
    def get_last_debug_info(self) -> Dict:
        """ë§ˆì§€ë§‰ ì‹¤í–‰ì˜ ë””ë²„ê·¸ ì •ë³´ ë°˜í™˜"""
        return self.last_debug_info
    
    def _create_prompt(self, user_query: str, context: str) -> str:
        """í–¥ìˆ˜ ì¶”ì²œì„ ìœ„í•œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if not context or context == "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.":
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ í–¥ìˆ˜ ì •ë³´ë§Œ ì¶”ì¶œ (ë””ë²„ê·¸ ì •ë³´ ì œê±°)
        lines = context.split('\n')
        perfume_info = []
        in_context_section = False
        
        for line in lines:
            # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì„¹ì…˜ ì‹œì‘ ê°ì§€
            if "ğŸ“‹ LLMì´ ë°›ëŠ” ì‹¤ì œ context" in line:
                in_context_section = True
                continue
            elif "ğŸ”— ë°œê²¬ëœ í–¥ë£Œ ì²´ì¸ íŒ¨í„´" in line or "=" in line:
                break
            elif in_context_section and line.strip() and line.startswith(('1.', '2.', '3.', '4.', '5.')):
                perfume_info.append(line.strip())
        
        clean_context = '\n'.join(perfume_info)
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í–¥ìˆ˜ ì •ë³´ ì§ì ‘ ì¶”ì¶œ
        if not clean_context.strip():
            print("âš ï¸  ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ - ì „ì²´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„")
            for line in lines:
                if ' for men' in line or ' for women' in line:
                    if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                        # í–¥ìˆ˜ ì •ë³´ë¥¼ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬
                        perfume_part = line.strip()
                        # "(ë¸Œëœë“œ:" ì´ì „ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        if ' (ë¸Œëœë“œ:' in perfume_part:
                            perfume_clean = perfume_part.split(' (ë¸Œëœë“œ:')[0]
                            perfume_info.append(perfume_clean)
                        else:
                            perfume_info.append(perfume_part)
                            
                if len(perfume_info) >= 5:
                    break
            
            clean_context = '\n'.join(perfume_info)
        
        print(f"ğŸ” ì¶”ì¶œëœ í–¥ìˆ˜ ì •ë³´ ({len(perfume_info)}ê°œ):")
        print(clean_context)
        print("-" * 40)
        
        # ê·¹ë„ë¡œ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
        return f"""ì§ˆë¬¸: {user_query}

# í–¥ìˆ˜ ëª©ë¡:
{clean_context}

#ìœ„ í–¥ìˆ˜ ì¤‘ì—ì„œ 3ê°œë¥¼ ì„ íƒí•´ì„œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
1. [ë¸Œëœë“œëª…] [í–¥ìˆ˜ëª…]
2. [ë¸Œëœë“œëª…] [í–¥ìˆ˜ëª…]  
3. [ë¸Œëœë“œëª…] [í–¥ìˆ˜ëª…]

# ë°˜ë“œì‹œ ìœ„ ëª©ë¡ì— ìˆëŠ” í–¥ìˆ˜ë§Œ ì„ íƒí•˜ì„¸ìš”.

# ë‹µë³€:
"""
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        if hasattr(self, 'pipeline') and self.pipeline:
            del self.pipeline
            gc.collect()
            torch.cuda.empty_cache()
        
        if hasattr(self, 'retrieval') and self.retrieval:
            self.retrieval.close()
        
        print("âœ… ì •ë¦¬ ì™„ë£Œ!")