{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef37100c",
   "metadata": {},
   "source": [
    "# Visualize your graph   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180e6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dibaeck/miniconda3/envs/cikm_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-VL-3B-Instruct ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dibaeck/miniconda3/envs/cikm_311/lib/python3.11/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/dibaeck/miniconda3/envs/cikm_311/lib/python3.11/site-packages/torch/cuda/__init__.py:1036: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 22795.13it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 30393.51it/s]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 29959.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded ✅\n",
      "Loading model: Qwen/Qwen2.5-VL-3B-Instruct ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dibaeck/miniconda3/envs/cikm_311/lib/python3.11/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 31536.12it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 32513.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded ✅\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from core.schemas import PipelineState\n",
    "from nodes.task_classifier import *\n",
    "from nodes.graph_nodes import graph_query_node\n",
    "from nodes.llm_response import llm_response_node\n",
    "from nodes.response_node import response_node\n",
    "from pipeline import build_pipeline_graph\n",
    "from core.schemas import PipelineState\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "app = build_pipeline_graph().compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae1507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```mermaid\n",
       "---\n",
       "config:\n",
       "  flowchart:\n",
       "    curve: linear\n",
       "---\n",
       "graph TD;\n",
       "\t__start__([<p>__start__</p>]):::first\n",
       "\tTASK_mapping(TASK_mapping)\n",
       "\tAgent_TaskClassification(Agent_TaskClassification)\n",
       "\tRetriver(Retriver)\n",
       "\tAgent_CheckingHop(Agent_CheckingHop)\n",
       "\tResponseFormatter(ResponseFormatter)\n",
       "\t__end__([<p>__end__</p>]):::last\n",
       "\tAgent_CheckingHop --> Retriver;\n",
       "\tAgent_TaskClassification --> Retriver;\n",
       "\tRetriver --> Agent_CheckingHop;\n",
       "\tRetriver --> ResponseFormatter;\n",
       "\tTASK_mapping --> Agent_TaskClassification;\n",
       "\t__start__ --> TASK_mapping;\n",
       "\tResponseFormatter --> __end__;\n",
       "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
       "\tclassDef first fill-opacity:0\n",
       "\tclassDef last fill:#bfb6fc\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mermaid_code = app.get_graph().draw_mermaid()\n",
    "display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cikm_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
