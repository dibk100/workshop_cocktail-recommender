from nodes.llm_task import llm_task_node
from nodes.graph_nodes import graph_query_node
from nodes.llm_response import llm_response_node
from nodes.response_node import response_node
from core.schemas import PipelineState

def run_pipeline(user_input: str) -> str:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ :
    user_input -> llm_task_node -> graph_query_node -> llm_response_node -> response_node
    """
    
    state = PipelineState(query=user_input)
    state = llm_task_node(state)
    state = graph_query_node(state)        # í˜„ì¬ëŠ” ë”ë¯¸ í•¨ìˆ˜
    state = llm_response_node(state)
    state = response_node(state)           # ìµœì¢… ì‘ë‹µ í¬ë§·íŒ…
    return state.final_text

def interface_cli():
    """
    CLI ê¸°ë°˜ ì¹µí…Œì¼ ì¶”ì²œ ì‹œìŠ¤í…œ í™”ë©´
    """
    print("\n=== ğŸ¸ Cocktail Recommendation System ğŸ¸ ===")
    print("Type your query to get a cocktail recommendation or type 'q' to quit.\n")

    while True:
        user_query = input("Your query: ").strip()

        if user_query.lower() in ["q", "quit", "exit"]:
            print("Exiting the system.")
            break

        if not user_query:
            print("Please enter a valid query.\n")
            continue

        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = run_pipeline(user_query)

        if not result:
            print("Failed to generate response. Exiting.")
            break

        print("\n=== Final Recommendation ===")
        print(result)
        print("\n" + "-"*50)

if __name__ == "__main__":
    interface_cli()